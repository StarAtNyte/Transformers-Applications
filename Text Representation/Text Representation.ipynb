{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7173b9f-2965-4417-9b34-18495183b48a",
   "metadata": {},
   "source": [
    "## Introduction to sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbef06b-46bb-4c3d-b1a8-e539f0ffd2e4",
   "metadata": {},
   "source": [
    "Pre-trained BERT models do not produce efficient and independent sentence embeddings\n",
    "as they always need to be fine-tuned in an end-to-end supervised setting. This is because\n",
    "we can think of a pre-trained BERT model as an indivisible whole and semantics is spread\n",
    "across all layers, not just the final layer. Without fine-tuning, it may be ineffective to use its\n",
    "internal representations independently. It is also hard to handle unsupervised tasks such\n",
    "as clustering, topic modeling, information retrieval, or semantic search. Because we have\n",
    "to evaluate many sentence pairs during clustering tasks, for instance, this causes massive\n",
    "computational overhead.\n",
    "\n",
    "Luckily, many modifications have been made to the original BERT model, such as Sentence-\n",
    "BERT (SBERT), to derive semantically meaningful and independent sentence embeddings.\n",
    "We will talk about these approaches in a moment. In the NLP literature, many neural\n",
    "sentence embedding methods have been proposed for mapping a single sentence to a\n",
    "common feature space (vector space model) wherein a cosine function (or dot product)\n",
    "is usually used to measure similarity and the Euclidean distance to measure dissimilarity.\n",
    "\n",
    "The following are some applications that can be efficiently solved with sentence embeddings:\n",
    "\n",
    "• Sentence-pair tasks\n",
    "\n",
    "• Information retrieval\n",
    "\n",
    "• Question answering\n",
    "\n",
    "• Duplicate question detection\n",
    "\n",
    "• Paraphrase detection\n",
    "\n",
    "• Document clustering\n",
    "\n",
    "• Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25abc27-06d1-45e0-8c46-84c4b7316799",
   "metadata": {},
   "source": [
    "## Benchmarking sentence similarity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fa306f-1017-4ab4-95d7-5436b86f9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/nitiz/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1455.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric, load_dataset\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "mrpc = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d665e2-ee92-456a-8c6e-3f47bfb4ab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson': -0.8660254037844388, 'spearmanr': -0.8660254037844387}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric('glue', 'stsb')\n",
    "metric.compute(predictions=[1,2,3],references=[5,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548954f1-8865-4832-b819-aec7d2c936f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from tensorflow-hub) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from tensorflow-hub) (4.23.0)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (4.29.2)\n",
      "Requirement already satisfied: tqdm in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\n",
      "Requirement already satisfied: requests in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: sympy in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: click in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=fbd7de60e4c879e32aff99d6bcb112db24305c74b2e2edefb65d60ea00cfaca1\n",
      "  Stored in directory: /home/nitiz/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d245a62b-f3a4-4462-a138-5a4dd2bfad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/stsb to /home/nitiz/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████| 803k/803k [00:00<00:00, 3.60MB/s]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/nitiz/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1002.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric, load_dataset\n",
    "stsb_metric = load_metric('glue', 'stsb')\n",
    "stsb = load_dataset('glue', 'stsb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c57a5d2-811d-48e1-ae02-5fcf97d06596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 20:58:46.032996: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-02 20:58:46.180786: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-02 20:58:46.181972: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 20:58:47.291507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-02 21:00:43.562143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-02 21:00:43.684527: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Downloading (…)e581e/.gitattributes: 100%|█| 1.09k/1.09k [00:00<00:00, 2.24MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████| 190/190 [00:00<00:00, 621kB/s]\n",
      "Downloading (…)8aa7be581e/README.md: 100%|█| 3.71k/3.71k [00:00<00:00, 4.51MB/s]\n",
      "Downloading (…)a7be581e/config.json: 100%|█████| 680/680 [00:00<00:00, 1.89MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████| 122/122 [00:00<00:00, 536kB/s]\n",
      "Downloading (…)aa7be581e/merges.txt: 100%|████| 456k/456k [00:01<00:00, 355kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 329M/329M [01:06<00:00, 4.90MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|████| 52.0/52.0 [00:00<00:00, 159kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 239/239 [00:00<00:00, 798kB/s]\n",
      "Downloading (…)e581e/tokenizer.json: 100%|██| 1.36M/1.36M [00:02<00:00, 530kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█| 1.12k/1.12k [00:00<00:00, 3.79MB/s]\n",
      "Downloading (…)aa7be581e/vocab.json: 100%|████| 798k/798k [00:01<00:00, 536kB/s]\n",
      "Downloading (…)7be581e/modules.json: 100%|██████| 229/229 [00:00<00:00, 704kB/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "distilroberta = SentenceTransformer('stsb-distilroberta-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7c9a76-7fd6-4974-a91c-5ad5f3a3b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "def use_sts_benchmark(batch):\n",
    "    sts_encode1 = tf.nn.l2_normalize(use_model(tf.constant(batch['sentence1'])), axis = 1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(use_model(tf.constant(batch['sentence2'])), axis = 1)\n",
    "    cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis = 1)\n",
    "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0,1.0)\n",
    "    scores = 1.0 -  tf.acos(clip_cosine_similarities) / math.pi\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a6007a5-6b18-47b1-b273-dbf4c67a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_sts_benchmark(batch):\n",
    "    sts_encode1 = tf.nn.l2_normalize(distilroberta.encode(batch['sentence1']), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(distilroberta.encode(batch['sentence2']), axis=1)\n",
    "    cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "    scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9653fc9-797f-454f-bfb0-161a38d4ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 21:03:36.777570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string\n",
      "\t [[{{node inputs}}]]\n"
     ]
    }
   ],
   "source": [
    "use_results = use_sts_benchmark(stsb['validation'])\n",
    "distilroberta_results = roberta_sts_benchmark(stsb['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21618b50-6e4c-42a3-a91e-ed4b7bf15c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [item['label'] for item in stsb['validation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa9bbc0-d06b-4f29-8bf5-31d266eb76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"USE\":stsb_metric.compute(\n",
    "        predictions=use_results,\n",
    "        references=references),\n",
    "\n",
    "    \"DistillRoberta\":stsb_metric.compute(\n",
    "        predictions=distilroberta_results,\n",
    "        references=references)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a70d57-0aff-457f-84ba-43afdfc8fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USE</th>\n",
       "      <th>DistillRoberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pearson</th>\n",
       "      <td>0.810301</td>\n",
       "      <td>0.888461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearmanr</th>\n",
       "      <td>0.808917</td>\n",
       "      <td>0.889246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                USE  DistillRoberta\n",
       "pearson    0.810301        0.888461\n",
       "spearmanr  0.808917        0.889246"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b6da7-cb1b-4ce4-9a49-33bb49613756",
   "metadata": {},
   "source": [
    "## Using BART for zero-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4654c901-ffe3-4e16-9c57-f3d1a0f4fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█| 1.15k/1.15k [00:00<00:00, 2.83MB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.63G/1.63G [02:36<00:00, 10.4MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████| 26.0/26.0 [00:00<00:00, 193kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 1.33MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|████| 456k/456k [00:00<00:00, 717kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 1.36M/1.36M [00:00<00:00, 1.44MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>travel</td>\n",
       "      <td>0.795756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>exploration</td>\n",
       "      <td>0.199332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>dancing</td>\n",
       "      <td>0.002621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>cooking</td>\n",
       "      <td>0.002291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sequence       labels    scores\n",
       "0  one day I will see the world       travel  0.795756\n",
       "1  one day I will see the world  exploration  0.199332\n",
       "2  one day I will see the world      dancing  0.002621\n",
       "3  one day I will see the world      cooking  0.002291"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                        model=\"facebook/bart-large-mnli\")\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel',\n",
    "                    'cooking',\n",
    "                    'dancing',\n",
    "                    'exploration']\n",
    "result = classifier(sequence_to_classify, candidate_labels)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d913bde-b4a8-420b-962a-e184e4a23e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>travel</td>\n",
       "      <td>0.994511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>exploration</td>\n",
       "      <td>0.938388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>dancing</td>\n",
       "      <td>0.005706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one day I will see the world</td>\n",
       "      <td>cooking</td>\n",
       "      <td>0.001819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sequence       labels    scores\n",
       "0  one day I will see the world       travel  0.994511\n",
       "1  one day I will see the world  exploration  0.938388\n",
       "2  one day I will see the world      dancing  0.005706\n",
       "3  one day I will see the world      cooking  0.001819"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier(sequence_to_classify,\n",
    "                    candidate_labels,\n",
    "                    multi_label=True)\n",
    "pd.DataFrame(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
