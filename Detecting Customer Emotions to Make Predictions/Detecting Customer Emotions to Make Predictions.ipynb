{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2953ecc-c57d-4a50-8422-03f7c1f0a93e",
   "metadata": {},
   "source": [
    "## Sentiment analysis with DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7a58b-07d0-4e5b-891c-aab34863ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitiz/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-31 21:14:53.119750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 21:14:53.907576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eeccce8-b380-42b3-9904-622d69af9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sequence, M):\n",
    "    nlp_cls = pipeline('sentiment-analysis')\n",
    "    if M==1:\n",
    "        print(nlp_cls.model.config)\n",
    "    return nlp_cls(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc69e496-5bde-4bd7-985f-27c8ddfc7294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The customer was very unhappy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████| 629/629 [00:00<00:00, 3.39MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 268M/268M [00:12<00:00, 21.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████| 48.0/48.0 [00:00<00:00, 363kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████| 232k/232k [00:00<00:00, 469kB/s]\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997097849845886}]\n"
     ]
    }
   ],
   "source": [
    "seq = 3\n",
    "if seq==1:\n",
    "    sequence=\"The battery on my Model9X phone doesn't last more than 6 hours and I'm unhappy about that.\"\n",
    "\n",
    "if seq==2:\n",
    "    sequence=\"The battery on my Model9X phone doesn't last more than 6 hours and I'm unhappy about that. I was really mad! I bought a Moel10x and things seem to be better. I'm super satisfied now.\"\n",
    "\n",
    "if seq==3:\n",
    "    sequence=\"The customer was very unhappy\"\n",
    "\n",
    "if seq==4:\n",
    "    sequence=\"The customer was very satisfied\"\n",
    "    \n",
    "print(sequence)\n",
    "M=1\n",
    "CS = classify(sequence,M)\n",
    "print(CS)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cbf15-b201-4cdc-b208-c360ae6f8940",
   "metadata": {},
   "source": [
    "## Sentiment analysis with RoBERTa-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce453ffd-2e34-4fc9-bce9-c6d03b0a76e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "class SimpleDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c297a594-b043-4530-994d-4ca5aeec26e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████| 256/256 [00:00<00:00, 652kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 687/687 [00:00<00:00, 2.39MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|████| 798k/798k [00:00<00:00, 962kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|████| 456k/456k [00:00<00:00, 730kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 150/150 [00:00<00:00, 445kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.42G/1.42G [01:08<00:00, 20.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"siebert/sentiment-roberta-large-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b0984e-9e7f-464d-88a1-11e3da3df549",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_texts = ['I like that','That is annoying','This is great!','Wouldn´t recommend it.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5481a5-7fb0-4bed-91e1-bf26621aa080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n",
    "pred_dataset = SimpleDataset(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e66fe816-1027-4703-a949-6bc9ff929806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run predictions\n",
    "predictions = trainer.predict(pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2467e73f-e6e3-42e0-8f8a-91f235fd0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-3.7254689,  2.8858626],\n",
       "       [ 3.9145916, -3.518445 ],\n",
       "       [-3.7518108,  2.9132538],\n",
       "       [ 3.9534545, -3.618488 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.9961, 'test_samples_per_second': 4.016, 'test_steps_per_second': 1.004})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7fd895-9f5a-4b0e-a841-2944ec169780",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = pd.Series(preds).map(model.config.id2label)\n",
    "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8441a99d-079b-46a2-9aa7-b74ed60590fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    POSITIVE\n",
       "1    NEGATIVE\n",
       "2    POSITIVE\n",
       "3    NEGATIVE\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01845729-b10c-4e6a-8eba-c13cc829c4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
